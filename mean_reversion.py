# -*- coding: utf-8 -*-
"""Mean Reversion

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IgtttW-TfVzJRO_C9NmmdCCwzR8Qx5-G
"""

import yfinance as yf
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import pytz
import matplotlib.pyplot as plt
import seaborn as sns
import time
import warnings
import matplotlib
matplotlib.use('Agg')  # Esto evita errores al generar gr√°ficos en servidores sin pantalla

# Suppress specific FutureWarning from yfinance
warnings.filterwarnings("ignore", category=FutureWarning, module='yfinance')

# 1. Configuraci√≥n de par√°metros
colombia_tz = pytz.timezone('America/Bogota')
ticker = "BTC-USD"
end = datetime.now()
#end = datetime(2026, 1, 21, 16, 0, tzinfo=colombia_tz) # Commented out future date
start = end - timedelta(days=365) # Changed to 1 year to get more data
interval = "1h"
window = 50 # Reduced window size for rolling calculations to avoid excessive NaNs
k = 2.5

# 2. Descarga de datos reales con reintentos
max_retries = 3
retry_delay_seconds = 60

for attempt in range(max_retries):
    try:
        data = yf.download(ticker, start=start, end=end, interval=interval)
        if not data.empty:
            break # Exit loop if data downloaded successfully
        else:
            print(f"Attempt {attempt + 1}: No data received from yfinance. Retrying...")
    except Exception as e:
        print(f"Attempt {attempt + 1}: Error downloading data: {e}")

    if attempt < max_retries - 1:
        print(f"Waiting for {retry_delay_seconds} seconds before retrying...")
        time.sleep(retry_delay_seconds)
    else:
        print("Max retries reached. Could not download data.")
        data = pd.DataFrame() # Ensure data is an empty DataFrame if all retries fail

if data.empty:
    print("No data available to process. Please check ticker, dates, or try again later.")
else:
    # Flatten MultiIndex columns if they exist (common with yfinance output for single tickers)
    if isinstance(data.columns, pd.MultiIndex):
        # Drop the second level which usually contains the ticker symbol (e.g., 'BTC-USD')
        data.columns = data.columns.droplevel(1)
        # Remove the name of the column level itself if present, for cleaner access
        data.columns.name = None

    data = data[['Open', 'High', 'Low', 'Close']].dropna()

    # Convertir el √≠ndice a la zona horaria de Colombia
    data.index = data.index.tz_convert(colombia_tz)

    # 3. C√°lculo de m√©tricas y anomal√≠as
    data['log_return'] = np.log(data['Close'] / data['Close'].shift(1))
    # Adjust window if 730 is too large for 6 months of 1h data (approx 4320 points)
    # If the window is larger than the available data points, this will result in NaNs
    effective_window = min(window, len(data) // 2) if len(data) > 0 else 1 # Ensure window is not too large
    if effective_window < window:
        print(f"Warning: Adjusted window for rolling calculations from {window} to {effective_window} due to limited data.")

    data['mean'] = data['log_return'].rolling(effective_window).mean()
    data['std'] = data['log_return'].rolling(effective_window).std()

    data['upper_band'] = data['mean'] + k * data['std']
    data['lower_band'] = data['mean'] - k * data['std']

    # Identificaci√≥n de anomal√≠as
    data['anomaly'] = (data['log_return'] > data['upper_band']) | (data['log_return'] < data['lower_band'])
    data['signal'] = np.where(data['log_return'] < data['lower_band'], 'LONG',
                            np.where(data['log_return'] > data['upper_band'], 'SHORT', None))
    anomalies = data[data['anomaly']].copy()

    # A√±adir la columna de se√±al de trading
    anomalies['signal'] = np.where(anomalies['log_return'] < anomalies['lower_band'], 'Long', 'Short')

import matplotlib.dates as mdates

# =============================
# Gr√°fico
# =============================
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True, gridspec_kw={'height_ratios': [1, 2]})

# Plot 1: Close Price
ax1.plot(data.index, data['Close'], label='Precio de Cierre', color='blue', alpha=0.8, linewidth=1)
ax1.set_title(f'An√°lisis de Anomal√≠as Horarias ‚Äì {ticker} (Hora Colombia)', fontsize=14)
ax1.set_ylabel('Precio de Cierre')
ax1.legend(loc='upper left')
ax1.grid(True, which='both', linestyle='--', alpha=0.5)

# Plot 2: Log Returns, Moving Average, Bands, and Anomalies
ax2.plot(data.index, data['log_return'],
         label='Log Returns (Horario)',
         alpha=0.6,
         linewidth=1)

ax2.plot(data.index, data['mean'],
         label='Media M√≥vil',
         linestyle='--',
         color='black')

ax2.plot(data.index, data['upper_band'],
         linestyle='dotted',
         color='gray',
         label=f'+{k}œÉ')

ax2.plot(data.index, data['lower_band'],
         linestyle='dotted',
         color='gray',
         label=f'-{k}œÉ')

ax2.scatter(anomalies.index,
            anomalies['log_return'],
            color='red',
            label='Anomal√≠as',
            zorder=5,
            s=20)

# Mejora del formato de fecha y hora en el eje X
ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
plt.xticks(rotation=45)

ax2.set_xlabel('Fecha y Hora (Colombia)') # Actualizar etiqueta del eje X
ax2.set_ylabel('Log Return')
ax2.legend()
ax2.grid(True, which='both', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

# 1.1. Definici√≥n de Features
# Usaremos una copia para no alterar el dataframe original de anomal√≠as
df_features = data.copy()
df_features['anomaly'] = data['anomaly']
# A√±adir la columna de se√±al de trading a df_features
df_features['signal'] = np.where(df_features['log_return'] < df_features['lower_band'], 'Long', 'Short')

# Feature 1: RSI (Relative Strength Index) - Captura sobrecompra/sobreventa
delta = df_features['Close'].diff()
gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
rs = gain / loss
df_features['RSI'] = 100 - (100 / (1 + rs))

# Feature 2: Volatilidad Hist√≥rica (Rolling Std de retornos)
df_features['Volatility'] = df_features['log_return'].rolling(window=24).std()

# Feature 3: Distancia respecto a la Media M√≥vil (Momentum)
df_features['Dist_from_MA'] = (df_features['Close'] - df_features['Close'].rolling(window=50).mean()) / df_features['Close'].rolling(window=50).mean()

# Feature 4: Rango de velas (High - Low) normalizado
df_features['HL_Range'] = (df_features['High'] - df_features['Low']) / df_features['Close']

# Feature 5: Cambio porcentual de volumen (si estuviera disponible)
# Como yf a veces da problemas con volumen, usaremos la aceleraci√≥n del log_return
df_features['Log_Return_Accel'] = df_features['log_return'].diff()

# A√±ade features rezagadas
df_features['RSI_lag1'] = df_features['RSI'].shift(1)
df_features['Volatility_lag1'] = df_features['Volatility'].shift(1)

# Feature extra: Fuerza de tendencia (ADX simplificado)
df_features['ADX'] = df_features['Close'].rolling(14).std() / df_features['Close'].rolling(50).std()

# Feature: Distancia a bandas de Bollinger (otro indicador de sobrecompra/sobreventa)
df_features['BB_upper'] = df_features['Close'].rolling(20).mean() + 2 * df_features['Close'].rolling(20).std()
df_features['BB_lower'] = df_features['Close'].rolling(20).mean() - 2 * df_features['Close'].rolling(20).std()
df_features['BB_position'] = (df_features['Close'] - df_features['BB_lower']) / (df_features['BB_upper'] - df_features['BB_lower'])

# Limpiamos los NaNs generados por las ventanas m√≥viles
df_features = df_features.dropna()

print("Features creadas exitosamente:")
print(df_features[['RSI', 'Volatility', 'Dist_from_MA', 'HL_Range', 'Log_Return_Accel']].tail())

# 2.1. Definici√≥n de Horizontes
horizontes = [4, 8, 12, 24, 48]

# 2.2. Creaci√≥n de columnas Target
# Para cada horizonte, miramos 'h' periodos hacia adelante
for h in horizontes:
    # Calculamos el retorno futuro acumulado
    future_return = df_features['Close'].shift(-h) / df_features['Close'] - 1

    # Clasificamos: 1 si el precio sube, 0 si baja
    df_features[f'target_{h}h'] = (future_return > 0).astype(int)

# Eliminamos las √∫ltimas filas que quedan con NaN en los targets
# (porque no hay futuro suficiente para calcular el horizonte de 48h)
df_features = df_features.dropna()

print(f"Targets generados para los horizontes: {horizontes}")
print(df_features[[f'target_{h}h' for h in horizontes]].tail())

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd
import warnings

# Suppress specific UserWarning from sklearn.utils.validation
warnings.filterwarnings("ignore", category=UserWarning, module='sklearn.utils.validation')

# ============================================
# 3.1. FEATURES OPTIMIZADAS (sin redundancia)
# ============================================

# Eliminamos BB_upper y BB_lower (redundantes con BB_position)
# Renombramos ADX a Vol_Ratio para mayor claridad
features_list = [
    'RSI',
    'Volatility',
    'Dist_from_MA',
    'HL_Range',
    'Log_Return_Accel',
    'RSI_lag1',
    'Volatility_lag1',
    'ADX',  # En realidad es Vol_Ratio pero mantenemos el nombre por compatibilidad
    'BB_position'
]

X = df_features[features_list]

# ============================================
# 3.2. CONFIGURACI√ìN DE MODELOS
# ============================================

mejores_modelos = {}
scalers = {}
metricas_detalladas = {}

# Par√°metros m√°s amplios para Grid Search
param_grid_knn = {
    'n_neighbors': [5, 7, 11, 15, 21],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Tambi√©n probamos Random Forest como alternativa
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [10, 20]
}

# ============================================
# 3.3. ENTRENAMIENTO CON WALK-FORWARD
# ============================================

print("=" * 60)
print("ENTRENAMIENTO DE MODELOS - WALK FORWARD VALIDATION")
print("=" * 60)

for h in horizontes:
    print(f"\n{'='*60}")
    print(f"HORIZONTE: {h} HORAS")
    print(f"{'='*60}")

    y = df_features[f'target_{h}h']

    # Split temporal: 80% train, 20% test
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

    print(f"üìä Train: {len(X_train)} observaciones | Test: {len(X_test)} observaciones")

    # Escalado
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ============================================
    # Cross-Validation con TimeSeriesSplit
    # ============================================
    tscv = TimeSeriesSplit(n_splits=5)

    print("\nüîç Probando KNN con Grid Search...")
    grid_knn = GridSearchCV(
        KNeighborsClassifier(),
        param_grid_knn,
        cv=tscv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=0
    )
    grid_knn.fit(X_train_scaled, y_train)

    print("\nüå≤ Probando Random Forest con Grid Search...")
    grid_rf = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid_rf,
        cv=tscv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=0
    )
    grid_rf.fit(X_train_scaled, y_train)

    # ============================================
    # Comparaci√≥n de modelos
    # ============================================
    knn_cv_score = grid_knn.best_score_
    rf_cv_score = grid_rf.best_score_

    knn_test_score = grid_knn.score(X_test_scaled, y_test)
    rf_test_score = grid_rf.score(X_test_scaled, y_test)

    print(f"\nüìà RESULTADOS COMPARATIVOS:")
    print(f"{'Modelo':<20} {'CV Score':<12} {'Test Score':<12} {'Diferencia':<12}")
    print(f"{'-'*60}")
    print(f"{'KNN':<20} {knn_cv_score:<12.4f} {knn_test_score:<12.4f} {abs(knn_cv_score - knn_test_score):<12.4f}")
    print(f"{'Random Forest':<20} {rf_cv_score:<12.4f} {rf_test_score:<12.4f} {abs(rf_cv_score - rf_test_score):<12.4f}")

    # Seleccionar mejor modelo basado en test score
    if knn_test_score >= rf_test_score:
        mejor_modelo = grid_knn.best_estimator_
        modelo_nombre = "KNN"
        mejor_score = knn_test_score
        mejores_params = grid_knn.best_params_
    else:
        mejor_modelo = grid_rf.best_estimator_
        modelo_nombre = "Random Forest"
        mejor_score = rf_test_score
        mejores_params = grid_rf.best_params_

    print(f"\nüèÜ GANADOR: {modelo_nombre}")
    print(f"üìã Par√°metros: {mejores_params}")

    # ============================================
    # M√©tricas detalladas en test set
    # ============================================
    y_pred = mejor_modelo.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    # Calcular baseline (siempre predecir la clase mayoritaria)
    baseline = y_test.value_counts().max() / len(y_test)

    print(f"\nüìä M√âTRICAS EN TEST SET:")
    print(f"{'M√©trica':<20} {'Valor':<12} {'vs Baseline':<12}")
    print(f"{'-'*60}")
    print(f"{'Accuracy':<20} {accuracy:<12.4f} {f'+{(accuracy - baseline)*100:.2f}%' if accuracy > baseline else f'{(accuracy - baseline)*100:.2f}%'}")
    print(f"{'Precision':<20} {precision:<12.4f}")
    print(f"{'Recall':<20} {recall:<12.4f}")
    print(f"{'F1-Score':<20} {f1:<12.4f}")
    print(f"{'Baseline (mayor√≠a)':<20} {baseline:<12.4f}")

    # ============================================
    # Re-entrenar con TODOS los datos para producci√≥n
    # ============================================
    print(f"\n‚ôªÔ∏è  Re-entrenando {modelo_nombre} con dataset completo...")
    scaler_final = StandardScaler()
    X_scaled_full = scaler_final.fit_transform(X)

    if modelo_nombre == "KNN":
        modelo_final = KNeighborsClassifier(**mejores_params)
    else:
        modelo_final = RandomForestClassifier(**mejores_params, random_state=42)

    modelo_final.fit(X_scaled_full, y)

    # Guardar modelo y scaler
    mejores_modelos[h] = modelo_final
    scalers[h] = scaler_final

    # Guardar m√©tricas
    metricas_detalladas[h] = {
        'modelo': modelo_nombre,
        'params': mejores_params,
        'cv_score': grid_knn.best_score_ if modelo_nombre == "KNN" else grid_rf.best_score_,
        'test_accuracy': accuracy,
        'test_precision': precision,
        'test_recall': recall,
        'test_f1': f1,
        'baseline': baseline,
        'mejora_vs_baseline': accuracy - baseline,
        'train_size': len(X_train),
        'test_size': len(X_test)
    }

# ============================================
# 3.4. RESUMEN FINAL
# ============================================

print("\n" + "="*60)
print("RESUMEN FINAL - TODOS LOS HORIZONTES")
print("="*60)

resumen_df = pd.DataFrame(metricas_detalladas).T
resumen_df.index.name = "Horizonte (h)"

print("\nüìä TABLA DE RENDIMIENTO:")
print(resumen_df[['modelo', 'test_accuracy', 'test_f1', 'mejora_vs_baseline']].to_string())

print("\nüéØ AN√ÅLISIS DE MEJORA:")
for h in horizontes:
    mejora = metricas_detalladas[h]['mejora_vs_baseline']
    if mejora > 0.05:
        estado = "üü¢ EXCELENTE"
    elif mejora > 0.02:
        estado = "üü° BUENO"
    elif mejora > 0:
        estado = "üü† MARGINAL"
    else:
        estado = "üî¥ POBRE"

    print(f"{h}h: {estado} (mejora: {mejora*100:+.2f}% vs baseline)")

print("\n‚úÖ Modelos entrenados y listos para producci√≥n")
print(f"üì¶ Features utilizadas: {len(features_list)}")
print(f"üéØ Horizontes configurados: {horizontes}")

# ============================================
# 3.5. ADVERTENCIAS Y RECOMENDACIONES
# ============================================

print("\n" + "="*60)
print("‚ö†Ô∏è  ADVERTENCIAS")
print("="*60)

for h in horizontes:
    mejora = metricas_detalladas[h]['mejora_vs_baseline']
    if mejora < 0.02:
        print(f"‚ö†Ô∏è  Horizonte {h}h: Mejora m√≠nima ({mejora*100:.2f}%). Considerar no operar este horizonte.")

    if metricas_detalladas[h]['test_accuracy'] < 0.52:
        print(f"‚ö†Ô∏è  Horizonte {h}h: Accuracy muy bajo ({metricas_detalladas[h]['test_accuracy']:.2%}). No recomendado para trading real.")

print("\nüí° RECOMENDACI√ìN:")
print("Solo opera en horizontes con mejora_vs_baseline > 2% y accuracy > 52%")
print("Usa KNN_score >= 60% como filtro adicional para se√±ales")

import numpy as np
import pandas as pd

# ============================================
# 4. PREDICCI√ìN EN TIEMPO REAL (√öLTIMA VELA)
# ============================================

print("=" * 60)
print("AN√ÅLISIS DE CONDICI√ìN ACTUAL")
print("=" * 60)

ultima_fila = df_features.tail(1)
es_anomalia_actual = ultima_fila['anomaly'].values[0]
fecha_actual = ultima_fila.index[0]
precio_actual = ultima_fila['Close'].values[0]

print(f"\nüìÖ Fecha/Hora: {fecha_actual}")
print(f"üí∞ Precio: ${precio_actual:,.2f}")
print(f"üìä Estado: {'üö® ANOMAL√çA DETECTADA' if es_anomalia_actual else '‚úÖ Mercado Normal'}")

if es_anomalia_actual:
    signal_mr = ultima_fila['signal'].values[0]
    log_return = ultima_fila['log_return'].values[0]

    print(f"üìà Log Return: {log_return:.4f}")
    print(f"üéØ Se√±al Mean Reversion: {signal_mr}")
    print(f"\n{'='*60}")
    print("PREDICCIONES KNN POR HORIZONTE")
    print(f"{'='*60}")

    X_actual = ultima_fila[features_list]
    predicciones = {}

    for h in horizontes:
        # Verificar si este horizonte es confiable
        if h in metricas_detalladas:
            mejora = metricas_detalladas[h]['mejora_vs_baseline']
            accuracy = metricas_detalladas[h]['test_accuracy']
            confiable = mejora > 0.02 and accuracy > 0.52
        else:
            confiable = True  # Si no hay m√©tricas, asumimos que s√≠

        # Escalar y predecir
        X_actual_scaled = scalers[h].transform(X_actual)
        pred = mejores_modelos[h].predict(X_actual_scaled)[0]
        prob = mejores_modelos[h].predict_proba(X_actual_scaled)[0]

        # Interpretaci√≥n
        tendencia = "ALCISTA" if pred == 1 else "BAJISTA"
        confianza = prob[pred]

        # Clasificaci√≥n de confianza
        if confianza >= 0.65:
            nivel_confianza = "üü¢ ALTA"
        elif confianza >= 0.55:
            nivel_confianza = "üü° MEDIA"
        else:
            nivel_confianza = "üî¥ BAJA"

        # Compatibilidad con mean reversion
        compatible = (tendencia == "ALCISTA" and signal_mr == "Long") or \
                     (tendencia == "BAJISTA" and signal_mr == "Short")

        predicciones[h] = {
            "Predicci√≥n": tendencia,
            "Confianza": f"{confianza:.1%}",
            "Nivel": nivel_confianza,
            "vs MeanRev": "‚úÖ Alineado" if compatible else "‚ö†Ô∏è Divergente",
            "Modelo Confiable": "‚úÖ" if confiable else "‚ùå"
        }

    # Mostrar tabla
    res_df = pd.DataFrame(predicciones).T
    res_df.index.name = "Horizonte (h)"
    print(res_df.to_string())

    # Consenso general
    print(f"\n{'='*60}")
    print("CONSENSO Y RECOMENDACI√ìN")
    print(f"{'='*60}")

    predicciones_alcistas = sum(1 for p in predicciones.values() if p["Predicci√≥n"] == "ALCISTA")
    total_horizontes = len(horizontes)
    consenso_pct = (predicciones_alcistas / total_horizontes) * 100

    if consenso_pct >= 80:
        consenso = f"üü¢ FUERTE ALCISTA ({consenso_pct:.0f}%)"
    elif consenso_pct >= 60:
        consenso = f"üü¢ ALCISTA ({consenso_pct:.0f}%)"
    elif consenso_pct >= 40:
        consenso = f"üü° NEUTRAL ({consenso_pct:.0f}%)"
    elif consenso_pct >= 20:
        consenso = f"üî¥ BAJISTA ({consenso_pct:.0f}%)"
    else:
        consenso = f"üî¥ FUERTE BAJISTA ({consenso_pct:.0f}%)"

    print(f"Consenso KNN: {consenso}")
    print(f"Se√±al Mean Reversion: {signal_mr}")

    # Recomendaci√≥n de trading
    compatible_count = sum(1 for p in predicciones.values() if p["vs MeanRev"] == "‚úÖ Alineado")
    alineamiento = (compatible_count / total_horizontes) * 100

    if alineamiento >= 60 and consenso_pct not in range(40, 61):
        print(f"\nüí° RECOMENDACI√ìN: ‚úÖ OPERAR")
        print(f"   Alineamiento KNN-MR: {alineamiento:.0f}%")
        print(f"   Direcci√≥n sugerida: {signal_mr}")
    else:
        print(f"\nüí° RECOMENDACI√ìN: ‚ö†Ô∏è ESPERAR")
        print(f"   Raz√≥n: Bajo alineamiento ({alineamiento:.0f}%) o se√±al neutral")

else:
    print("\nüí° No se requiere an√°lisis detallado.")
    print("   El mercado opera dentro de par√°metros normales.")

# ============================================
# 5. AN√ÅLISIS HIST√ìRICO DE ANOMAL√çAS
# ============================================

print("\n" + "=" * 60)
print("AN√ÅLISIS HIST√ìRICO - SOLO ANOMAL√çAS")
print("=" * 60)

anomalias_historicas = df_features[df_features['anomaly']].copy()
print(f"\nüìä Total de anomal√≠as detectadas: {len(anomalias_historicas)}")
print(f"üìä Porcentaje del dataset: {len(anomalias_historicas)/len(df_features)*100:.2f}%")

# Generar predicciones para todas las anomal√≠as
resultados_knn = {h: [] for h in horizontes}

for idx, row in anomalias_historicas.iterrows():
    X_anomalia = row[features_list].values.reshape(1, -1)

    for h in horizontes:
        X_scaled = scalers[h].transform(X_anomalia)
        pred = mejores_modelos[h].predict(X_scaled)[0]
        label = "ALCISTA" if pred == 1 else "BAJISTA"
        resultados_knn[h].append(label)

# Integrar resultados
for h in horizontes:
    anomalias_historicas[f'KNN_{h}h'] = resultados_knn[h]

# ============================================
# 6. REPORTE FINAL CON M√âTRICAS CORREGIDAS
# ============================================

reporte_final = anomalias_historicas.copy()

# Normalizar se√±al Mean Reversion
reporte_final['signal'] = np.where(
    reporte_final['signal'] == "Long", "LONG", "SHORT"
)

reporte_final['signal_dir'] = np.where(
    reporte_final['signal'] == "LONG", "ALCISTA", "BAJISTA"
)

# ============================================
# 6.1 ALINEAMIENTO KNN vs MEAN REVERSION POR ANOMAL√çA
# ============================================

print("\n" + "=" * 60)
print("ALINEAMIENTO KNN vs MEAN REVERSION POR ANOMAL√çA")
print("=" * 60)

# Calculate alignment for each horizon per anomaly
alignment_checks_per_horizon = pd.DataFrame()
for h in horizontes:
    alignment_checks_per_horizon[f'Align_{h}h'] = (
        reporte_final[f'KNN_{h}h'] == reporte_final['signal_dir']
    ).astype(int)

# Calculate the average alignment score across all horizons for each anomaly
reporte_final['MR_Alignment_Score'] = alignment_checks_per_horizon.mean(axis=1) * 100
print("MR_Alignment_Score a√±adido al reporte final. Este score indica el porcentaje de horizontes en los que el KNN predice la misma direcci√≥n que la se√±al de Mean Reversion para esa anomal√≠a especifica.")

# Categorizar el MR_Alignment_Score
bins = [0, 21, 41, 61, 81, 101]
labels = ['‚ö™ Basura','üî¥Bajo', 'üü†Medio', 'üü°Bueno', 'üü¢Excelente']
reporte_final['MR_Alignment_Category'] = pd.cut(reporte_final['MR_Alignment_Score'], bins=bins, labels=labels, right=False)


# ============================================
# 6.2 PRECISI√ìN DEL MODELO (vs Realidad)
# ============================================
print("\n" + "=" * 60)
print("PRECISI√ìN DEL MODELO (vs Realidad)")
print("=" * 60)

for h in horizontes:
    pred = reporte_final[f'KNN_{h}h']
    target = reporte_final[f'target_{h}h']

    # Check vs realidad (LO √öNICO QUE IMPORTA)
    reporte_final[f'Check_{h}h'] = np.where(
        ((pred == "ALCISTA") & (target == 1)) |
        ((pred == "BAJISTA") & (target == 0)),
        "‚úÖ", "‚ùå"
    )

    # Calcular m√©tricas
    aciertos = (reporte_final[f'Check_{h}h'] == "‚úÖ").sum()
    total = len(reporte_final)
    accuracy = (aciertos / total) * 100

    # Baseline: siempre predecir la clase mayoritaria
    baseline = reporte_final[f'target_{h}h'].value_counts().max() / total * 100
    mejora = accuracy - baseline

    # Clasificaci√≥n
    if mejora >= 5:
        estado = "üü¢ EXCELENTE"
    elif mejora >= 2:
        estado = "üü° BUENO"
    elif mejora >= 0:
        estado = "üü† MARGINAL"
    else:
        estado = "üî¥ POBRE"

    print(f"{h}h: {accuracy:.2f}% | Baseline: {baseline:.2f}% | Mejora: {mejora:+.2f}% {estado}")

# ============================================
# 6.3 TABLA FINAL SIMPLIFICADA
# ============================================

# Add MR_Alignment_Score to the columns displayed
columnas_finales = ['Close', 'signal', 'MR_Alignment_Score', 'MR_Alignment_Category', 'log_return']

for h in horizontes:
    columnas_finales.extend([f'KNN_{h}h', f'Check_{h}h'])

print("=" * 60)
print("√öLTIMAS 15 ANOMAL√çAS")
print("=" * 60)
print(reporte_final[columnas_finales].tail(15).to_string(index=True))

# ============================================
# 6.4 DISTRIBUCI√ìN DE SE√ëALES EN ANOMAL√çAS
# ============================================

print("\n" + "=" * 60)
print("DISTRIBUCI√ìN DE SE√ëALES EN ANOMAL√çAS")
print("=" * 60)

dist_signal = reporte_final['signal'].value_counts()
print(f"\nSe√±al Mean Reversion:")
print(f"  LONG:  {dist_signal.get('LONG', 0)} ({dist_signal.get('LONG', 0)/len(reporte_final)*100:.1f}%)")
print(f"  SHORT: {dist_signal.get('SHORT', 0)} ({dist_signal.get('SHORT', 0)/len(reporte_final)*100:.1f}%)")

for h in horizontes:
    dist_knn = reporte_final[f'KNN_{h}h'].value_counts()
    print(f"\nKNN {h}h:")
    print(f"  ALCISTA: {dist_knn.get('ALCISTA', 0)} ({dist_knn.get('ALCISTA', 0)/len(reporte_final)*100:.1f}%)")
    print(f"  BAJISTA: {dist_knn.get('BAJISTA', 0)} ({dist_knn.get('BAJISTA', 0)/len(reporte_final)*100:.1f}%)")

# ============================================
# 6.5 RECOMENDACIONES FINALES
# ============================================

print("\n" + "=" * 60)
print("RECOMENDACIONES PARA TRADING")
print("=" * 60)

print("\n‚úÖ CRITERIOS PARA OPERAR:")
print("   1. Anomal√≠a detectada (signal = LONG o SHORT)")
print("   2. KNN con mejora > 2% vs baseline")
print("   3. Confianza del modelo >= 60%")
print("   4. Alineamiento KNN-MR >= 60% (opcional)")

print("\n‚ö†Ô∏è  GESTI√ìN DE RIESGO:")
print("   ‚Ä¢ Stop loss: -2% del precio de entrada")
print("   ‚Ä¢ Take profit: Seg√∫n horizonte √≥ptimo")
print("   ‚Ä¢ Position size: M√°ximo 5% del capital por trade")
print("   ‚Ä¢ No operar m√°s de 3 posiciones simult√°neas")

print("\nüîç PR√ìXIMOS PASOS:")
print("   1. Implementar backtesting con P&L real")
print("   2. Calcular Sharpe ratio y max drawdown")
print("   3. Paper trading por 30 d√≠as")
print("   4. Ajustar par√°metros seg√∫n resultados")



import requests



import os

def enviar_telegram(mensaje):
    bot_token = os.getenv('BOT_TOKEN')
    chat_id = os.getenv('CHAT_ID')
    if not bot_token or not chat_id:
        print("Error: BOT_TOKEN o CHAT_ID no est√°n configurados.")
        return
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": mensaje,
        "parse_mode": "Markdown"
    }
    response = requests.post(url, data=payload)
    if response.status_code != 200:
        print(f"Error al enviar mensaje a Telegram: {response.text}")
    else:
        print("Mensaje enviado correctamente a Telegram.")

# Asegurarse de que df_features, ultima_fila, y las variables relacionadas est√©n disponibles
# Si este bloque se ejecuta de forma independiente, podr√≠a necesitar cargar los datos nuevamente o asegurar que df_features existe.
# Para este contexto, asumimos que las celdas anteriores ya han ejecutado y estas variables est√°n en el kernel.

# Re-calcular la √∫ltima fila para asegurar que los datos sean los m√°s recientes si el kernel se reinici√≥.
# En un flujo continuo, estas variables ya deber√≠an estar disponibles.
ultima_fila = df_features.tail(1)
es_anomalia_actual = ultima_fila['anomaly'].values[0]
fecha_actual = ultima_fila.index[0]
precio_actual = ultima_fila['Close'].values[0]

mensaje_telegram_final = f"""
üìä *An√°lisis de Mercado Actual*

üìÖ Fecha/Hora: {fecha_actual}
üí∞ Precio: ${precio_actual:,.2f}
üìà Estado: {'üö® ANOMAL√çA DETECTADA' if es_anomalia_actual else '‚úÖ Mercado Normal'}
"""

# Si hay una anomal√≠a, a√±adir m√°s detalles al mensaje
if es_anomalia_actual:
    signal_mr = ultima_fila['signal'].values[0]
    log_return = ultima_fila['log_return'].values[0]

    X_actual = ultima_fila[features_list]
    predicciones = {}

    # Recalcular predicciones y confianzas para la √∫ltima vela
    for h in horizontes:
        # Verificar si este horizonte es confiable (usando metricas_detalladas del contexto)
        if h in metricas_detalladas:
            mejora = metricas_detalladas[h]['mejora_vs_baseline']
            accuracy = metricas_detalladas[h]['test_accuracy']
            confiable_modelo = mejora > 0.02 and accuracy > 0.52
        else:
            confiable_modelo = False # Si no hay m√©tricas, asumimos no confiable

        # Escalar y predecir
        X_actual_scaled = scalers[h].transform(X_actual)
        pred = mejores_modelos[h].predict(X_actual_scaled)[0]
        prob = mejores_modelos[h].predict_proba(X_actual_scaled)[0]

        tendencia = "ALCISTA" if pred == 1 else "BAJISTA"
        confianza = prob[pred] if pred < len(prob) else 0.0 # Asegurar √≠ndice v√°lido

        nivel_confianza = ""
        if confianza >= 0.65:
            nivel_confianza = "üü¢ ALTA"
        elif confianza >= 0.55:
            nivel_confianza = "üü° MEDIA"
        else:
            nivel_confianza = "üî¥ BAJA"

        # Normalizar signal_mr para comparaci√≥n
        normalized_signal_mr = "ALCISTA" if signal_mr == "Long" else "BAJISTA"
        compatible = (tendencia == normalized_signal_mr) # Comparar predicci√≥n KNN con se√±al MR normalizada

        predicciones[h] = {
            "Predicci√≥n": tendencia,
            "Confianza": confianza,
            "Nivel": nivel_confianza,
            "vs MeanRev": "‚úÖ Alineado" if compatible else "‚ö†Ô∏è Divergente",
            "Modelo Confiable": "‚úÖ" if confiable_modelo else "‚ùå"
        }

    # Calcular MR_Alignment_Score para la anomal√≠a actual
    normalized_signal_mr_text = "ALCISTA" if signal_mr == "Long" else "BAJISTA"
    alignment_scores = []
    for h_key, p_val in predicciones.items():
        if p_val["Predicci√≥n"] == normalized_signal_mr_text:
            alignment_scores.append(1)
        else:
            alignment_scores.append(0)
    current_mr_alignment_score = (sum(alignment_scores) / len(alignment_scores)) * 100 if alignment_scores else 0

    # Categorizar el MR_Alignment_Score
    bins = [0, 21, 41, 61, 81, 101]
    labels = ['‚ö™ Basura','üî¥Bajo', 'üü†Medio', 'üü°Bueno', 'üü¢Excelente']
    current_mr_alignment_category = pd.cut([current_mr_alignment_score], bins=bins, labels=labels, right=False)[0]

    # A√±adir detalles al mensaje_telegram_final
    mensaje_telegram_final += f"""

üéØ Se√±al Mean Reversion: {signal_mr}
ü§ù Alineamiento KNN-MR: {current_mr_alignment_score:.0f}% ({current_mr_alignment_category})
"""

    mensaje_telegram_final += f"\n*PREDICCIONES POR HORIZONTE:*\n"
    predicciones_summary_lines = []
    for h, p_data in predicciones.items():
        # Simplificado seg√∫n la petici√≥n del usuario
        predicciones_summary_lines.append(
            f"  *{h}h:* Pred: {p_data['Predicci√≥n']}"
        )
    mensaje_telegram_final += "\n".join(predicciones_summary_lines)


else:
    mensaje_telegram_final += "\nüí° No se requiere an√°lisis detallado.\n   El mercado opera dentro de par√°metros normales.\n"

# Enviar el mensaje por Telegram
enviar_telegram(mensaje_telegram_final)

